{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "## Final Data Processing\n",
    "\n",
    "Want to understand the full experimental design? Here's the process, we apologize in advance.\n",
    "________\n",
    "\n",
    "#### **Part 1: Definitions**\n",
    "\n",
    "This notebook walks through the final data processing for identifying \"valid observations\" that can be used in the final regression analysis. A \"valid observation\" is defined as a *collection of posts* from a single profile that matches the required pattern for either a control observation or a treatment observation. The following definitions wil be used throughout the notebook and are the basis for this causal experimental design.\n",
    "\n",
    "- **Viral Post**: A single post whose (square root transformed) engagement exceeds 3 standard deviations from the profile mean engagement.\n",
    "\n",
    "- **Baseline**: A series of `w`consecutive posts where the (square root transformed) engagement for every post falls within 2 standard deviations of the mean. A baseline is expected to represent a level of consistent engagement for the profile's posts. \n",
    "\n",
    "- **Window (`w`)**: Required number of consecutive posts to define a valid \"baseline\". The window is varied between 5 and 40 posts to perform a sensitivity analysis on the consistency of the results. \n",
    "\n",
    "- **Treatment Observation**: A treatment observation is represented as the combination of a single (or many) viral posts surrounded by a baseline before and after the instance(s) of virality. Data from each baseline before and after the viral post will be submitted as individual post data to the regression analysis. For example, if the window `n` is equal to 20 posts, a \"valid treatment observation\" with a single viral post between baselines would submit 41 samples of data to the regression. However, in total, all 41 samples would be evaluating the effect of a single observation of virality. Hence, we call it a single treatment observation.\n",
    "\n",
    "- **Control Observation**: If we define a treatment observation to be virality samdwiched between two baselines, we define a the pattern for control observations to be no viral posts sandwiched between two baselines. In other words, two consecutive baselines stitched together for a total baseline length of `2*n`.\n",
    "\n",
    "To minimize the chances of observing seasonality effects for a particular profile (say a brand known for summer-time clothing that only experiences virality during the summer and only experiences control observations during winter), treatment observations and control observations are taken from pairs of companies matched based on followers, engagement levels, and other profile-level evaluation metrics. By splitting companies into treatment and control, we also prevent potential data leakage from residual effects of treatment data found in control (or vice versa). Company matching is explained in another notebook. At this point we are provided a list of 10 matched control and treatment profiles from which we'll search for valid control and treatment observations.\n",
    "_______\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 2: Extracting Control and Treatment Observations**\n",
    "\n",
    "The objective of this notebook is to search each of the treatment profile engagement data and find valid instances of \"baseline-viral-baseline\" patterns for each of the respective window sizes `w`. For this explanation we'll use window_size of `20 posts`. The high level process and associated functions are described below.\n",
    "\n",
    "\n",
    ">1. Sort Data by 'Date' and calculate appropriate standard deviation thresholds.\n",
    ">    \n",
    ">    Functions:\n",
    ">    - sort_data()\n",
    ">    - calculate_thresholds()\n",
    "\n",
    "*Treatment Process*:\n",
    "\n",
    ">2. Find and store indices of viral posts.\n",
    ">    \n",
    ">    Functions:\n",
    ">    - find_virality()\n",
    "\n",
    ">3. Find valid pairs of baselines (pre- and post- baseline with a gap in between). Overlapping data is not permitted - each instance of virality must have a unique pre and post baseline. Therefore, for treatment, the first and last identified baseline for a particular profile can be `w=20` posts in length, but any baseline in the middle must be twice as long to adequately represent a *post-baseline* for the previous virality and a *pre-baseline* for the subsequent virality. To maintain consistency between measuring baselines for treatment and control, the baselines are limited to window size `w=20` posts. Therefore, if a profile starts the year with 100 consecutive posts at baseline levels before a viral post, the pre-baseline data will include only the last `w=20` posts before virality. This enables the \"control observation\" (of length `2*w`) to have the same size pre-baseline and post-baseline as the treatment observations.\n",
    ">\n",
    ">    Functions:\n",
    ">    - find_treatment_baselines()\n",
    "\n",
    ">4. For each pair of pre- and post- baselines, evaluate whether there is (are) any viral post(s) between the pairs of pre- and post- baselines. If there is at least one viral post, store the indices for pre-baseline, post-baseline, and number of viral posts as a single treatment observation.\n",
    ">\n",
    ">    Functions:\n",
    ">    - find_valid_treatment_obs()\n",
    "\n",
    ">5. Using the indices stored for the starts and ends of the treatment observation baselines, extract these rows of data (individual posts) from the main dataset and tag them with binary indicator for before or after virality `Time_frame = 0/1` along with the variable number of viral posts associated with the baselines `Treatment = V`. By using the number of viral posts between baselines, we quantify the treatment intensity and derive a *per-viral-post* treatment effect from the eventual interaction term coefficient between `Time_Frame` and `Treatment`.\n",
    ">\n",
    ">    Functions:\n",
    ">    - extract_observations()\n",
    "\n",
    "The process is repeated for identifying the control observations, but instead of searching for isolated baselines with unique strings of `w=20` posts, we search for consecutive baselines of `2w=40` posts, then split the singular baseline into a simulated *pre-baseline* and *post-baseline* both of length `w=20` posts. These control observations will control for time-based natural fluctuations in engagement and represent \"a world without virality\" for the match paired treatment company. Similar to before, the time frame is set to `Time_Frame = 0/1` and the Treatment is set to `Treatment = 0`.\n",
    "\n",
    "At this point we have two dataframes: Treatment DataFrame and Control DataFrame - observations extracted from their respective treatment/control matched company pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### **Part 3: Matching... Again?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we chose to match companies together into control and treatment groups, we end up with different quantities of treatment and control observations. Not only is the number different, but the distribution of valid observations across matched company pairs is different for control companies and treatment companies. In order to fix this discrepancy, we need to match again, but this time match treatment observations directly to control observations within the matched company pairs. \n",
    "\n",
    "Descriptively, this means we can only use the minimum number of observations found between the control and treatment company of each matched pair. By doing this, we ensure the same number of total control and treatment observations for the regression and we ensure the control group and treatment group have the same distribution of company type/size representation. \n",
    "\n",
    "Unfortunately this method reduces the number of valid observations even farther than what we've already pruned. But we believe this step was necessary to ensure balanced representation between the treatment and control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "#### **Part 4: Merging**\n",
    "\n",
    "The final step in this notebook is to merge the final dataset with additional contrl variables that will be used in the regression. \n",
    "\n",
    "- Month - Month that it was posted\n",
    "- Post type - Instagram TV, Reel, Photo\n",
    "- Number of Posts - Company based metric to define the posting frequency for the company\n",
    "- Number of Followers - Company based metric to define the size of the profile\n",
    "\n",
    "When all the data is merged, a single CSV file is output with all treatment and control observations. Parts 1-4 are iterated for window_sizes between 5 posts and 40 posts.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(df):\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by='Date')\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns='index', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_treatment_baselines(df, base_col, w):\n",
    "\n",
    "    #Copy and shift the baseline column down one space\n",
    "    shifted = df[base_col].shift(1)\n",
    "\n",
    "    # Find all baseline starts\n",
    "    baseline_starts = (df[base_col] - shifted)==1\n",
    "\n",
    "    # Find all baseline ends\n",
    "    baseline_ends = (df[base_col] - shifted)==-1\n",
    "\n",
    "    # Adjust start based on window size\n",
    "    starts = [ind - (w-1) for ind in baseline_starts[baseline_starts].index]\n",
    "\n",
    "    # Adjust end to exclude the post that broke the baseline\n",
    "    ends = [ind - 1 for ind in baseline_ends[baseline_ends].index]\n",
    "\n",
    "    # If the last index is still part of a baseline, end the baseline at the last index.\n",
    "    last_ind = len(df[base_col]) - 1\n",
    "    if len(starts) > len(ends):\n",
    "        ends.append(last_ind)\n",
    "\n",
    "    # Create list of tuples for baseline starts and ends\n",
    "    baselines = [(starts[i], ends[i]) for i in range(len(starts))]\n",
    "\n",
    "    # Prevent overlapping pre-baselines and post-baselines\n",
    "    # If there are less than 2 baselines, we cannot establish a pre-viral and post-viral baseline\n",
    "    if len(baselines) < 2:\n",
    "        return []\n",
    "    \n",
    "    else:\n",
    "        first_baseline = [baselines[0]]\n",
    "        last_baseline = [baselines[-1]]\n",
    "        b2b_baselines = [t for i, t in enumerate(baselines) if 0 < i < len(baselines)-1 and t[1] - t[0] >= 2*w]\n",
    "    \n",
    "    # Final list of acceptable baselines\n",
    "    baselines = first_baseline + b2b_baselines + last_baseline\n",
    "\n",
    "    # Modify baselines to be capped at the window size length\n",
    "    final_baselines = []\n",
    "\n",
    "    for i in range(len(baselines)-1):\n",
    "\n",
    "        _, pre_base_end = baselines[i]\n",
    "        pre_base_st = pre_base_end - (w-1)\n",
    "        pre_base = (pre_base_st, pre_base_end)\n",
    "\n",
    "        post_base_st, _ = baselines[i+1]\n",
    "        post_base_end = post_base_st + (w-1)\n",
    "        post_base = (post_base_st, post_base_end)\n",
    "\n",
    "        final_baselines.append((pre_base, post_base))\n",
    "\n",
    "    return final_baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_virality(df, viral_col):\n",
    "    # viral_col = 'log_over_3SD'\n",
    "    viral_cond = (df[viral_col] == 1)\n",
    "    viral_posts = [ind for ind in viral_cond[viral_cond].index]\n",
    "    return viral_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_treatment_obs(df, window_sizes):\n",
    "    # Iterate through the different consecutive posts required for baseline\n",
    "    results = {}\n",
    "    for w in window_sizes:\n",
    "        results[w] = {}\n",
    "        baseline_column = f'baseline_{w}'\n",
    "        baseline_eval_column = 'sqrt_under_2SD'\n",
    "        viral_column = 'sqrt_over_3SD'\n",
    "\n",
    "        # Create binary indicator column for baseline\n",
    "        df[baseline_column] = df[baseline_eval_column].rolling(window=w, min_periods=w).sum() == w\n",
    "\n",
    "        # Find indices for viral posts\n",
    "        viral_posts = find_virality(df, viral_column)\n",
    "\n",
    "        # Return tuples for start and end of baselines all baselines for this company\n",
    "        baselines = find_treatment_baselines(df, baseline_column, w)\n",
    "\n",
    "        # Set up variables for number of (valid) observations for this company (baseline-viral-baseline combos)\n",
    "        results[w][\"valid_obs\"] = []\n",
    "        results[w][\"num_valid_obs\"] = 0\n",
    "\n",
    "        # Calculate number of viral posts between baselines\n",
    "        valid_obs_temp = []\n",
    "        for b in baselines:\n",
    "            (start_lower, end_lower), (start_upper, end_upper) = b\n",
    "            viral_count = sum([1 for num in viral_posts if end_lower < num < start_upper])\n",
    "            obs = ((start_lower, end_lower), (start_upper, end_upper), viral_count)\n",
    "\n",
    "            # if viral post is found between baselines, observation is valid and can be used in regression\n",
    "            if viral_count > 0:\n",
    "                valid_obs_temp.append(obs)\n",
    "        \n",
    "        # Save valid observations associated with current window size\n",
    "        results[w][\"valid_obs\"] = valid_obs_temp\n",
    "        results[w][\"num_valid_obs\"] = len(valid_obs_temp)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_observations(valid_obs, df, treatment):\n",
    "    \n",
    "    new_observations = pd.DataFrame(columns=list(df.columns))\n",
    "    \n",
    "    for obs in valid_obs:\n",
    "\n",
    "        (lower_base_st, lower_base_end), (upper_base_st, upper_base_end), vposts = obs\n",
    "        lower_base = df.loc[lower_base_st:lower_base_end].copy()\n",
    "        upper_base = df.loc[upper_base_st:upper_base_end].copy()\n",
    "\n",
    "        # Assign NPV and TF\n",
    "        if treatment:\n",
    "            lower_base['Time_Frame'] = 0\n",
    "            lower_base['Treatment'] = vposts\n",
    "            upper_base['Time_Frame'] = 1\n",
    "            upper_base['Treatment'] = vposts\n",
    "        else:\n",
    "            lower_base['Time_Frame'] = 0\n",
    "            lower_base['Treatment'] = 0\n",
    "            upper_base['Time_Frame'] = 1\n",
    "            upper_base['Treatment'] = 0\n",
    "\n",
    "        # Concatenate baselines to final dataframes \n",
    "        new_observations = pd.concat([new_observations, lower_base, upper_base])\n",
    "\n",
    "    return new_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_thresholds(df):\n",
    "    # Square Root Engagement\n",
    "    df['sqrt_engagement'] = np.sqrt(df['Engagement'])\n",
    "\n",
    "    # Z-Score Transformation\n",
    "    df['sqrt_engagement_zscore'] = (df['sqrt_engagement'] - df['sqrt_engagement'].mean()) / df['sqrt_engagement'].std()\n",
    "    #df['z_score_mean'] = df['sqrt_engagement_zscore'].mean()\n",
    "    #df['z_score_std'] = df['sqrt_engagement_zscore'].std()\n",
    "\n",
    "    # Engagment Z-Score\n",
    "    df['engagement_zscore'] = (df['Engagement'] - df['Engagement'].mean()) / df['Engagement'].std()\n",
    "\n",
    "    # Mean and standard deviation for engagement across entire year\n",
    "    df['sqrt_mean'] = df['engagement_zscore'].mean()\n",
    "    df['sqrt_stdev'] = df['engagement_zscore'].std()\n",
    "\n",
    "    # 2SD and 3SD thresholds\n",
    "    df['3sd_sqrt'] = df['sqrt_mean'] + df['sqrt_stdev']*3\n",
    "    df['2sd_sqrt'] = df['sqrt_mean'] + df['sqrt_stdev']*2\n",
    "\n",
    "    # Binary indicator for post within 2SD (baseline)\n",
    "    df['sqrt_under_2SD'] = df['engagement_zscore'] <= df['2sd_sqrt']\n",
    "\n",
    "    # Binary indicator for post over 3SD (viral)\n",
    "    df['sqrt_over_3SD'] = df['engagement_zscore'] >= df['3sd_sqrt']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_control_baselines(df, base_col, w):\n",
    "\n",
    "    #Copy and shift the baseline column down one space\n",
    "    shifted = df[base_col].shift(1)\n",
    "\n",
    "    # Find all baseline starts\n",
    "    baseline_starts = (df[base_col] - shifted)==1\n",
    "\n",
    "    # Find all baseline ends\n",
    "    baseline_ends = (df[base_col] - shifted)==-1\n",
    "\n",
    "    # Adjust start based on window size\n",
    "    starts = [ind - (w-1) for ind in baseline_starts[baseline_starts].index]\n",
    "\n",
    "    # Adjust end to exclude the post that broke the baseline\n",
    "    ends = [ind - 1 for ind in baseline_ends[baseline_ends].index]\n",
    "\n",
    "    # If the last index is still part of a baseline, end the baseline at the last index.\n",
    "    last_ind = len(df[base_col]) - 1\n",
    "    if len(starts) > len(ends):\n",
    "        ends.append(last_ind)\n",
    "\n",
    "    # Create list of tuples for baseline starts and ends\n",
    "    baselines = [(starts[i], ends[i]) for i in range(len(starts))]\n",
    "    final_baselines = []\n",
    "    for b in baselines:\n",
    "        start_ind, end_ind = b\n",
    "        divisions = (end_ind - start_ind + 1)//(2*w)\n",
    "        for _ in range(divisions):\n",
    "            pre_base_start = start_ind\n",
    "            pre_base_end = start_ind + w - 1\n",
    "            pre_base = (pre_base_start, pre_base_end)\n",
    "            post_base_start = pre_base_end + 1\n",
    "            post_base_end = post_base_start + w - 1\n",
    "            post_base = (post_base_start, post_base_end)\n",
    "\n",
    "            final_baselines.append((pre_base, post_base))\n",
    "            start_ind = post_base_end+1\n",
    "\n",
    "    return final_baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_control_obs(df, window_sizes):\n",
    "    # Iterate through the different consecutive posts required for baseline\n",
    "    results = {}\n",
    "    for w in window_sizes:\n",
    "        results[w] = {}\n",
    "        baseline_column = f'baseline_{w}'\n",
    "        baseline_eval_column = 'sqrt_under_2SD'\n",
    "        viral_column = 'sqrt_over_3SD'\n",
    "\n",
    "        # Create binary indicator column for baseline\n",
    "        df[baseline_column] = df[baseline_eval_column].rolling(window=w, min_periods=w).sum() == w\n",
    "\n",
    "        # Find indices for viral posts\n",
    "        viral_posts = find_virality(df, viral_column)\n",
    "\n",
    "        # Return tuples for start and end of baselines all baselines for this company\n",
    "        baselines = find_control_baselines(df, baseline_column, w)\n",
    "\n",
    "        # Set up variables for number of (valid) observations for this company (baseline-viral-baseline combos)\n",
    "        results[w][\"valid_obs\"] = []\n",
    "        results[w][\"num_valid_obs\"] = 0\n",
    "\n",
    "        # Calculate number of viral posts between baselines\n",
    "        valid_obs_temp = []\n",
    "        for b in baselines:\n",
    "            (start_lower, end_lower), (start_upper, end_upper) = b\n",
    "            viral_count = sum([1 for num in viral_posts if end_lower < num < start_upper])\n",
    "            obs = ((start_lower, end_lower), (start_upper, end_upper), viral_count)\n",
    "\n",
    "            # all baselines returned from the find_control_baselines function are valid\n",
    "            valid_obs_temp.append(obs)\n",
    "        \n",
    "        # Save valid observations associated with current window size\n",
    "        results[w][\"valid_obs\"] = valid_obs_temp\n",
    "        results[w][\"num_valid_obs\"] = len(valid_obs_temp)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Set pandas dataframe max columns/rows\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up final treatment and control dataframes for different window sizes\n",
    "\n",
    "treatment3df = pd.DataFrame()\n",
    "treatment5df = pd.DataFrame()\n",
    "treatment10df = pd.DataFrame()\n",
    "treatment20df = pd.DataFrame()\n",
    "treatment30df = pd.DataFrame()\n",
    "treatment40df = pd.DataFrame()\n",
    "treatment50df = pd.DataFrame()\n",
    "treatment_dfs = {\n",
    "    3: treatment3df,\n",
    "    5: treatment5df,\n",
    "    10: treatment10df,\n",
    "    20: treatment20df,\n",
    "    30: treatment30df,\n",
    "    40: treatment40df,\n",
    "    50: treatment50df\n",
    "}\n",
    "\n",
    "control3df = pd.DataFrame()\n",
    "control5df = pd.DataFrame()\n",
    "control10df = pd.DataFrame()\n",
    "control20df = pd.DataFrame()\n",
    "control30df = pd.DataFrame()\n",
    "control40df = pd.DataFrame()\n",
    "control50df = pd.DataFrame()\n",
    "control_dfs = {\n",
    "    3: control3df,\n",
    "    5: control5df,\n",
    "    10: control10df,\n",
    "    20: control20df,\n",
    "    30: control30df,\n",
    "    40: control40df,\n",
    "    50: control50df\n",
    "}\n",
    "\n",
    "\n",
    "# Set up treatment and control companies\n",
    "treatment_companies = ['adidasoriginals', 'allbirds', 'asos', 'everlane', 'ganni', 'goat', 'lunya', 'pomelofashion', 'prettylittlething', 'fashionphile']\n",
    "control_companies = ['vinted', 'urbanic', 'stockx', 'lenskart', 'mejuri', 'veja', 'rixo', 'printful', 'skims', 'primark']\n",
    "\n",
    "# Set up windows sizes\n",
    "window_sizes = [5,10,20,30,40]\n",
    "window_sizes_str = [str(w) for w in window_sizes]\n",
    "\n",
    "# Set up dataframes to track posts used in the regression per company (PPC)\n",
    "summarization_metrics = ['total_posts', 'total_companies_contributed','contribution_mean', 'contribution_stdev']\n",
    "treatment_PPC = pd.DataFrame(columns = treatment_companies + summarization_metrics, index = window_sizes_str)\n",
    "control_PPC = pd.DataFrame(columns = control_companies + summarization_metrics, index = window_sizes_str)\n",
    "\n",
    "\n",
    "# Set up dataframe to describe company contributions\n",
    "matched_companies = [f'{treatment_companies[i]} : {control_companies[i]}' for i in range(len(treatment_companies))]\n",
    "matched_contribution_ratio = pd.DataFrame(columns = matched_companies, index = window_sizes_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Treatment Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data csv\n",
    "file_path = \"data\\SocialInsider_Instagram_posts.csv\"\n",
    "main_df = pd.read_csv(file_path)\n",
    "\n",
    "for company in treatment_companies:\n",
    "    df = main_df[main_df['Profile'] == company].copy()\n",
    "    df = sort_data(df)\n",
    "    df = calculate_thresholds(df) # 2/3 standard deviation levels\n",
    "    valid_observations = find_valid_treatment_obs(df, window_sizes) # baseline-viral-baseline\n",
    "\n",
    "    for w in window_sizes:\n",
    "\n",
    "        treatment_dfs[w]\n",
    "        valid_obs = valid_observations[w]['valid_obs']\n",
    "        new_observations = extract_observations(valid_obs=valid_obs, df=df, treatment=True)\n",
    "        treatment_PPC.loc[str(w), company] = len(new_observations)\n",
    "        treatment_dfs[w] = pd.concat([treatment_dfs[w], new_observations], ignore_index=True)\n",
    "\n",
    "# Create a dataframe for number of posts that can be used for each combination company and window size\n",
    "treatment_subset_df = treatment_PPC.iloc[:, 0:10]\n",
    "treatment_PPC['total_companies_contributed'] = treatment_subset_df.apply(lambda row: row[row != 0].count(), axis=1)\n",
    "treatment_PPC['total_posts'] = treatment_subset_df.apply(lambda row: row[row != 0].sum(), axis=1)\n",
    "treatment_PPC['contribution_mean'] = treatment_subset_df.mean(axis=1)\n",
    "treatment_PPC['contribution_stdev'] = treatment_subset_df.std(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adidasoriginals</th>\n",
       "      <th>allbirds</th>\n",
       "      <th>asos</th>\n",
       "      <th>everlane</th>\n",
       "      <th>ganni</th>\n",
       "      <th>goat</th>\n",
       "      <th>lunya</th>\n",
       "      <th>pomelofashion</th>\n",
       "      <th>prettylittlething</th>\n",
       "      <th>fashionphile</th>\n",
       "      <th>total_posts</th>\n",
       "      <th>total_companies_contributed</th>\n",
       "      <th>contribution_mean</th>\n",
       "      <th>contribution_stdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>530</td>\n",
       "      <td>10</td>\n",
       "      <td>53.0</td>\n",
       "      <td>48.773855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>220</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "      <td>80.0</td>\n",
       "      <td>76.594169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>120</td>\n",
       "      <td>240</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>880</td>\n",
       "      <td>10</td>\n",
       "      <td>88.0</td>\n",
       "      <td>74.951836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60</td>\n",
       "      <td>120</td>\n",
       "      <td>180</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>240</td>\n",
       "      <td>60</td>\n",
       "      <td>960</td>\n",
       "      <td>10</td>\n",
       "      <td>96.0</td>\n",
       "      <td>64.498062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>80</td>\n",
       "      <td>1120</td>\n",
       "      <td>9</td>\n",
       "      <td>112.0</td>\n",
       "      <td>85.997416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adidasoriginals allbirds asos everlane ganni goat lunya pomelofashion  \\\n",
       "5               10       30  150       10    60   10    60            80   \n",
       "10              20       60  220       20    40   20    60           160   \n",
       "20              40      120  240       40    40   40    40            80   \n",
       "30              60      120  180       60    60   60    60            60   \n",
       "40              80      160  160       80    80   80    80             0   \n",
       "\n",
       "   prettylittlething fashionphile  total_posts  total_companies_contributed  \\\n",
       "5                110           10          530                           10   \n",
       "10               180           20          800                           10   \n",
       "20               200           40          880                           10   \n",
       "30               240           60          960                           10   \n",
       "40               320           80         1120                            9   \n",
       "\n",
       "    contribution_mean  contribution_stdev  \n",
       "5                53.0           48.773855  \n",
       "10               80.0           76.594169  \n",
       "20               88.0           74.951836  \n",
       "30               96.0           64.498062  \n",
       "40              112.0           85.997416  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_PPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Control Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data csv\n",
    "file_path = \"data\\SocialInsider_Instagram_posts.csv\"\n",
    "main_df = pd.read_csv(file_path)\n",
    "\n",
    "for company in control_companies:\n",
    "    df = main_df[main_df['Profile'] == company].copy()\n",
    "    df = sort_data(df)\n",
    "    df = calculate_thresholds(df)\n",
    "    valid_observations = find_valid_control_obs(df, window_sizes)\n",
    "    \n",
    "    for w in window_sizes:\n",
    "\n",
    "        valid_obs = valid_observations[w]['valid_obs']\n",
    "        new_observations = extract_observations(valid_obs=valid_obs, df=df, treatment=False)\n",
    "        control_PPC.loc[str(w), company] = len(new_observations)\n",
    "        control_dfs[w] = pd.concat([control_dfs[w], new_observations], ignore_index=True)\n",
    "    \n",
    "control_subset_df = control_PPC.iloc[:, 0:10]\n",
    "control_PPC['total_companies_contributed'] = control_subset_df.apply(lambda row: row[row != 0].count(), axis=1)\n",
    "control_PPC['total_posts'] = control_subset_df.apply(lambda row: row[row != 0].sum(), axis=1)\n",
    "control_PPC['contribution_mean'] = control_subset_df.mean(axis=1)\n",
    "control_PPC['contribution_stdev'] = control_subset_df.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vinted</th>\n",
       "      <th>urbanic</th>\n",
       "      <th>stockx</th>\n",
       "      <th>lenskart</th>\n",
       "      <th>mejuri</th>\n",
       "      <th>veja</th>\n",
       "      <th>rixo</th>\n",
       "      <th>printful</th>\n",
       "      <th>skims</th>\n",
       "      <th>primark</th>\n",
       "      <th>total_posts</th>\n",
       "      <th>total_companies_contributed</th>\n",
       "      <th>contribution_mean</th>\n",
       "      <th>contribution_stdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>110</td>\n",
       "      <td>40</td>\n",
       "      <td>220</td>\n",
       "      <td>810</td>\n",
       "      <td>370</td>\n",
       "      <td>260</td>\n",
       "      <td>740</td>\n",
       "      <td>200</td>\n",
       "      <td>1230</td>\n",
       "      <td>890</td>\n",
       "      <td>4870</td>\n",
       "      <td>10</td>\n",
       "      <td>487.0</td>\n",
       "      <td>400.501075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>180</td>\n",
       "      <td>780</td>\n",
       "      <td>340</td>\n",
       "      <td>160</td>\n",
       "      <td>560</td>\n",
       "      <td>200</td>\n",
       "      <td>1120</td>\n",
       "      <td>780</td>\n",
       "      <td>4220</td>\n",
       "      <td>10</td>\n",
       "      <td>422.0</td>\n",
       "      <td>368.836007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>720</td>\n",
       "      <td>320</td>\n",
       "      <td>40</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>920</td>\n",
       "      <td>600</td>\n",
       "      <td>3000</td>\n",
       "      <td>9</td>\n",
       "      <td>300.0</td>\n",
       "      <td>330.521473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>720</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>720</td>\n",
       "      <td>360</td>\n",
       "      <td>2400</td>\n",
       "      <td>8</td>\n",
       "      <td>240.0</td>\n",
       "      <td>274.226184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>640</td>\n",
       "      <td>320</td>\n",
       "      <td>1920</td>\n",
       "      <td>5</td>\n",
       "      <td>192.0</td>\n",
       "      <td>259.092433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vinted urbanic stockx lenskart mejuri veja rixo printful skims primark  \\\n",
       "5     110      40    220      810    370  260  740      200  1230     890   \n",
       "10     80      20    180      780    340  160  560      200  1120     780   \n",
       "20     40       0     40      720    320   40  160      160   920     600   \n",
       "30     60       0     60      720    120    0  180      180   720     360   \n",
       "40      0       0      0      640    160    0    0      160   640     320   \n",
       "\n",
       "    total_posts  total_companies_contributed  contribution_mean  \\\n",
       "5          4870                           10              487.0   \n",
       "10         4220                           10              422.0   \n",
       "20         3000                            9              300.0   \n",
       "30         2400                            8              240.0   \n",
       "40         1920                            5              192.0   \n",
       "\n",
       "    contribution_stdev  \n",
       "5           400.501075  \n",
       "10          368.836007  \n",
       "20          330.521473  \n",
       "30          274.226184  \n",
       "40          259.092433  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_PPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "**Minimum Usable Posts based on observation matched pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adidasoriginals : vinted</th>\n",
       "      <th>allbirds : urbanic</th>\n",
       "      <th>asos : stockx</th>\n",
       "      <th>everlane : lenskart</th>\n",
       "      <th>ganni : mejuri</th>\n",
       "      <th>goat : veja</th>\n",
       "      <th>lunya : rixo</th>\n",
       "      <th>pomelofashion : printful</th>\n",
       "      <th>prettylittlething : skims</th>\n",
       "      <th>fashionphile : primark</th>\n",
       "      <th>total_companies_contributed</th>\n",
       "      <th>total_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>240</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adidasoriginals : vinted allbirds : urbanic asos : stockx  \\\n",
       "5                        10                 30           150   \n",
       "10                       20                 20           180   \n",
       "20                       40                  0            40   \n",
       "30                       60                  0            60   \n",
       "40                        0                  0             0   \n",
       "\n",
       "   everlane : lenskart ganni : mejuri goat : veja lunya : rixo  \\\n",
       "5                   10             60          10           60   \n",
       "10                  20             40          20           60   \n",
       "20                  40             40          40           40   \n",
       "30                  60             60           0           60   \n",
       "40                  80             80           0            0   \n",
       "\n",
       "   pomelofashion : printful prettylittlething : skims fashionphile : primark  \\\n",
       "5                        80                       110                     10   \n",
       "10                      160                       180                     20   \n",
       "20                       80                       200                     40   \n",
       "30                       60                       240                     60   \n",
       "40                        0                       320                     80   \n",
       "\n",
       "    total_companies_contributed  total_posts  \n",
       "5                            10          530  \n",
       "10                           10          720  \n",
       "20                            9          560  \n",
       "30                            8          660  \n",
       "40                            4          560  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = control_subset_df\n",
    "df2 = treatment_subset_df\n",
    "min_df = pd.DataFrame(np.minimum(df1.values, df2.values), columns=matched_companies, index=df1.index)\n",
    "min_copy = min_df.copy()\n",
    "min_df['total_companies_contributed'] = min_copy.apply(lambda row: row[row != 0].count(), axis=1)\n",
    "min_df['total_posts'] = min_copy.apply(lambda row: row[row != 0].sum(), axis=1)\n",
    "\n",
    "min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "**How many viral posts will be evaluated for each combination company/window_size?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adidasoriginals : vinted</th>\n",
       "      <th>allbirds : urbanic</th>\n",
       "      <th>asos : stockx</th>\n",
       "      <th>everlane : lenskart</th>\n",
       "      <th>ganni : mejuri</th>\n",
       "      <th>goat : veja</th>\n",
       "      <th>lunya : rixo</th>\n",
       "      <th>pomelofashion : printful</th>\n",
       "      <th>prettylittlething : skims</th>\n",
       "      <th>fashionphile : primark</th>\n",
       "      <th>total_viral_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adidasoriginals : vinted allbirds : urbanic asos : stockx  \\\n",
       "5                       1.0                3.0          15.0   \n",
       "10                      1.0                1.0           9.0   \n",
       "20                      1.0                0.0           1.0   \n",
       "30                      1.0                0.0           1.0   \n",
       "40                      0.0                0.0           0.0   \n",
       "\n",
       "   everlane : lenskart ganni : mejuri goat : veja lunya : rixo  \\\n",
       "5                  1.0            6.0         1.0          6.0   \n",
       "10                 1.0            2.0         1.0          3.0   \n",
       "20                 1.0            1.0         1.0          1.0   \n",
       "30                 1.0            1.0         0.0          1.0   \n",
       "40                 1.0            1.0         0.0          0.0   \n",
       "\n",
       "   pomelofashion : printful prettylittlething : skims fashionphile : primark  \\\n",
       "5                       8.0                      11.0                    1.0   \n",
       "10                      8.0                       9.0                    1.0   \n",
       "20                      2.0                       5.0                    1.0   \n",
       "30                      1.0                       4.0                    1.0   \n",
       "40                      0.0                       4.0                    1.0   \n",
       "\n",
       "    total_viral_posts  \n",
       "5                53.0  \n",
       "10               36.0  \n",
       "20               14.0  \n",
       "30               11.0  \n",
       "40                7.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_series = pd.Series(window_sizes)\n",
    "total_viral_df = min_copy.div(window_sizes, axis=0)\n",
    "total_viral_df = total_viral_df/2\n",
    "total_viral_df['total_viral_posts'] = total_viral_df.apply(lambda row: row[row != 0].sum(), axis=1)\n",
    "total_viral_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to treatment and control dictionary. Can reuse code because there will be the same number of observations for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vinted': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 1.0, '40': 0.0},\n",
       " 'urbanic': {'5': 3.0, '10': 1.0, '20': 0.0, '30': 0.0, '40': 0.0},\n",
       " 'stockx': {'5': 15.0, '10': 9.0, '20': 1.0, '30': 1.0, '40': 0.0},\n",
       " 'lenskart': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 1.0, '40': 1.0},\n",
       " 'mejuri': {'5': 6.0, '10': 2.0, '20': 1.0, '30': 1.0, '40': 1.0},\n",
       " 'veja': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 0.0, '40': 0.0},\n",
       " 'rixo': {'5': 6.0, '10': 3.0, '20': 1.0, '30': 1.0, '40': 0.0},\n",
       " 'printful': {'5': 8.0, '10': 8.0, '20': 2.0, '30': 1.0, '40': 0.0},\n",
       " 'skims': {'5': 11.0, '10': 9.0, '20': 5.0, '30': 4.0, '40': 4.0},\n",
       " 'primark': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 1.0, '40': 1.0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_num_baselines = total_viral_df.iloc[:,0:10].copy()\n",
    "control_num_baselines.columns = df1.columns\n",
    "control_num_baselines.index = df1.index\n",
    "ctrl_dict = control_num_baselines.to_dict()\n",
    "ctrl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adidasoriginals': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 1.0, '40': 0.0},\n",
       " 'allbirds': {'5': 3.0, '10': 1.0, '20': 0.0, '30': 0.0, '40': 0.0},\n",
       " 'asos': {'5': 15.0, '10': 9.0, '20': 1.0, '30': 1.0, '40': 0.0},\n",
       " 'everlane': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 1.0, '40': 1.0},\n",
       " 'ganni': {'5': 6.0, '10': 2.0, '20': 1.0, '30': 1.0, '40': 1.0},\n",
       " 'goat': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 0.0, '40': 0.0},\n",
       " 'lunya': {'5': 6.0, '10': 3.0, '20': 1.0, '30': 1.0, '40': 0.0},\n",
       " 'pomelofashion': {'5': 8.0, '10': 8.0, '20': 2.0, '30': 1.0, '40': 0.0},\n",
       " 'prettylittlething': {'5': 11.0, '10': 9.0, '20': 5.0, '30': 4.0, '40': 4.0},\n",
       " 'fashionphile': {'5': 1.0, '10': 1.0, '20': 1.0, '30': 1.0, '40': 1.0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treat_num_baselines = total_viral_df.iloc[:,0:10].copy()\n",
    "treat_num_baselines.columns = df2.columns\n",
    "treat_num_baselines.index = df2.index\n",
    "treat_num_baselines.to_dict()\n",
    "treat_dict = treat_num_baselines.to_dict()\n",
    "treat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "**Selected matched observations for control and treatment**\n",
    "\n",
    "Re-run previous code, but limit output for valid observations to the maximum number allowable posts identified in the dictionaries above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up final treatment and control dataframes for observations that will be used in the regression\n",
    "# Different dataframe for each of the different window sizes\n",
    "\n",
    "treatment5df = pd.DataFrame()\n",
    "treatment10df = pd.DataFrame()\n",
    "treatment20df = pd.DataFrame()\n",
    "treatment30df = pd.DataFrame()\n",
    "treatment40df = pd.DataFrame()\n",
    "treatment_dfs = {\n",
    "    5: treatment5df,\n",
    "    10: treatment10df,\n",
    "    20: treatment20df,\n",
    "    30: treatment30df,\n",
    "    40: treatment40df\n",
    "}\n",
    "\n",
    "control5df = pd.DataFrame()\n",
    "control10df = pd.DataFrame()\n",
    "control20df = pd.DataFrame()\n",
    "control30df = pd.DataFrame()\n",
    "control40df = pd.DataFrame()\n",
    "control_dfs = {\n",
    "    5: control5df,\n",
    "    10: control10df,\n",
    "    20: control20df,\n",
    "    30: control30df,\n",
    "    40: control40df\n",
    "}\n",
    "\n",
    "\n",
    "# Set up treatment and control companies\n",
    "treatment_companies = ['adidasoriginals', 'allbirds', 'asos', 'everlane', 'ganni', 'goat', 'lunya', 'pomelofashion', 'prettylittlething', 'fashionphile']\n",
    "control_companies = ['vinted', 'urbanic', 'stockx', 'lenskart', 'mejuri', 'veja', 'rixo', 'printful', 'skims', 'primark']\n",
    "\n",
    "# Set up windows sizes\n",
    "window_sizes = [5,10,20,30,40]\n",
    "window_sizes_str = [str(w) for w in window_sizes]\n",
    "\n",
    "# Set up dataframes to track posts used in the regression per company (PPC)\n",
    "summarization_metrics = ['total_posts', 'total_companies_contributed','contribution_mean', 'contribution_stdev']\n",
    "treatment_PPC = pd.DataFrame(columns = treatment_companies + summarization_metrics, index = window_sizes_str)\n",
    "control_PPC = pd.DataFrame(columns = control_companies + summarization_metrics, index = window_sizes_str)\n",
    "\n",
    "\n",
    "# Set up dataframe to describe company contributions\n",
    "matched_companies = [f'{treatment_companies[i]} : {control_companies[i]}' for i in range(len(treatment_companies))]\n",
    "matched_contribution_ratio = pd.DataFrame(columns = matched_companies, index = window_sizes_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Control Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Read all data csv\n",
    "file_path = \"data\\SocialInsider_Instagram_posts.csv\"\n",
    "main_df = pd.read_csv(file_path)\n",
    "\n",
    "number = 0\n",
    "\n",
    "for company in control_companies:\n",
    "    df = main_df[main_df['Profile'] == company].copy()\n",
    "    df = sort_data(df)\n",
    "    df = calculate_thresholds(df)\n",
    "    valid_observations = find_valid_control_obs(df, window_sizes)\n",
    "    \n",
    "    for w in window_sizes:\n",
    "\n",
    "        valid_obs = valid_observations[w]['valid_obs']\n",
    "        str_w = str(w)\n",
    "        n = int(ctrl_dict[company][str_w])\n",
    "        valid_obs = random.sample(valid_obs, n)\n",
    "        new_observations = extract_observations(valid_obs=valid_obs, df=df, treatment=False)\n",
    "        control_PPC.loc[str(w), company] = len(new_observations)\n",
    "        control_dfs[w] = pd.concat([control_dfs[w], new_observations], ignore_index=True)\n",
    "    \n",
    "control_subset_df = control_PPC.iloc[:, 0:10]\n",
    "control_PPC['total_companies_contributed'] = control_subset_df.apply(lambda row: row[row != 0].count(), axis=1)\n",
    "control_PPC['total_posts'] = control_subset_df.apply(lambda row: row[row != 0].sum(), axis=1)\n",
    "control_PPC['contribution_mean'] = control_subset_df.mean(axis=1)\n",
    "control_PPC['contribution_stdev'] = control_subset_df.std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Treatment Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data csv\n",
    "file_path = \"data\\SocialInsider_Instagram_posts.csv\"\n",
    "main_df = pd.read_csv(file_path)\n",
    "\n",
    "for company in treatment_companies:\n",
    "    df = main_df[main_df['Profile'] == company].copy()\n",
    "    df = sort_data(df)\n",
    "    df = calculate_thresholds(df)\n",
    "    valid_observations = find_valid_treatment_obs(df, window_sizes)\n",
    "\n",
    "    for w in window_sizes:\n",
    "\n",
    "        valid_obs = valid_observations[w]['valid_obs']\n",
    "        str_w = str(w)\n",
    "        n = int(treat_dict[company][str_w])\n",
    "        valid_obs = random.sample(valid_obs, n)\n",
    "        new_observations = extract_observations(valid_obs=valid_obs, df=df, treatment=True)\n",
    "        treatment_PPC.loc[str(w), company] = len(new_observations)\n",
    "        treatment_dfs[w] = pd.concat([treatment_dfs[w], new_observations], ignore_index=True)\n",
    "\n",
    "treatment_subset_df = treatment_PPC.iloc[:, 0:10]\n",
    "treatment_PPC['total_companies_contributed'] = treatment_subset_df.apply(lambda row: row[row != 0].count(), axis=1)\n",
    "treatment_PPC['total_posts'] = treatment_subset_df.apply(lambda row: row[row != 0].sum(), axis=1)\n",
    "treatment_PPC['contribution_mean'] = treatment_subset_df.mean(axis=1)\n",
    "treatment_PPC['contribution_stdev'] = treatment_subset_df.std(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Treatment PPC and Control PPC**\n",
    "These dataframes should be the same for each matched company now. Matched companies are paired by column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vinted</th>\n",
       "      <th>urbanic</th>\n",
       "      <th>stockx</th>\n",
       "      <th>lenskart</th>\n",
       "      <th>mejuri</th>\n",
       "      <th>veja</th>\n",
       "      <th>rixo</th>\n",
       "      <th>printful</th>\n",
       "      <th>skims</th>\n",
       "      <th>primark</th>\n",
       "      <th>total_posts</th>\n",
       "      <th>total_companies_contributed</th>\n",
       "      <th>contribution_mean</th>\n",
       "      <th>contribution_stdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>530</td>\n",
       "      <td>10</td>\n",
       "      <td>53.0</td>\n",
       "      <td>48.773855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>720</td>\n",
       "      <td>10</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.305290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>560</td>\n",
       "      <td>9</td>\n",
       "      <td>56.0</td>\n",
       "      <td>53.995885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>240</td>\n",
       "      <td>60</td>\n",
       "      <td>660</td>\n",
       "      <td>8</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.030296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>80</td>\n",
       "      <td>560</td>\n",
       "      <td>4</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100.133245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vinted urbanic stockx lenskart mejuri veja rixo printful skims primark  \\\n",
       "5      10      30    150       10     60   10   60       80   110      10   \n",
       "10     20      20    180       20     40   20   60      160   180      20   \n",
       "20     40       0     40       40     40   40   40       80   200      40   \n",
       "30     60       0     60       60     60    0   60       60   240      60   \n",
       "40      0       0      0       80     80    0    0        0   320      80   \n",
       "\n",
       "    total_posts  total_companies_contributed  contribution_mean  \\\n",
       "5           530                           10               53.0   \n",
       "10          720                           10               72.0   \n",
       "20          560                            9               56.0   \n",
       "30          660                            8               66.0   \n",
       "40          560                            4               56.0   \n",
       "\n",
       "    contribution_stdev  \n",
       "5            48.773855  \n",
       "10           71.305290  \n",
       "20           53.995885  \n",
       "30           66.030296  \n",
       "40          100.133245  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_PPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adidasoriginals</th>\n",
       "      <th>allbirds</th>\n",
       "      <th>asos</th>\n",
       "      <th>everlane</th>\n",
       "      <th>ganni</th>\n",
       "      <th>goat</th>\n",
       "      <th>lunya</th>\n",
       "      <th>pomelofashion</th>\n",
       "      <th>prettylittlething</th>\n",
       "      <th>fashionphile</th>\n",
       "      <th>total_posts</th>\n",
       "      <th>total_companies_contributed</th>\n",
       "      <th>contribution_mean</th>\n",
       "      <th>contribution_stdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "      <td>530</td>\n",
       "      <td>10</td>\n",
       "      <td>53.0</td>\n",
       "      <td>48.773855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>720</td>\n",
       "      <td>10</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.305290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>560</td>\n",
       "      <td>9</td>\n",
       "      <td>56.0</td>\n",
       "      <td>53.995885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>240</td>\n",
       "      <td>60</td>\n",
       "      <td>660</td>\n",
       "      <td>8</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.030296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>80</td>\n",
       "      <td>560</td>\n",
       "      <td>4</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100.133245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adidasoriginals allbirds asos everlane ganni goat lunya pomelofashion  \\\n",
       "5               10       30  150       10    60   10    60            80   \n",
       "10              20       20  180       20    40   20    60           160   \n",
       "20              40        0   40       40    40   40    40            80   \n",
       "30              60        0   60       60    60    0    60            60   \n",
       "40               0        0    0       80    80    0     0             0   \n",
       "\n",
       "   prettylittlething fashionphile  total_posts  total_companies_contributed  \\\n",
       "5                110           10          530                           10   \n",
       "10               180           20          720                           10   \n",
       "20               200           40          560                            9   \n",
       "30               240           60          660                            8   \n",
       "40               320           80          560                            4   \n",
       "\n",
       "    contribution_mean  contribution_stdev  \n",
       "5                53.0           48.773855  \n",
       "10               72.0           71.305290  \n",
       "20               56.0           53.995885  \n",
       "30               66.0           66.030296  \n",
       "40               56.0          100.133245  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_PPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "#### **Merging**\n",
    "\n",
    "Concatenate treatment observations with control observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged5df = pd.concat([control_dfs[5], treatment_dfs[5]], ignore_index=True)\n",
    "merged10df = pd.concat([control_dfs[10], treatment_dfs[10]], ignore_index=True)\n",
    "merged20df = pd.concat([control_dfs[20], treatment_dfs[20]], ignore_index=True)\n",
    "merged30df = pd.concat([control_dfs[30], treatment_dfs[30]], ignore_index=True)\n",
    "merged40df = pd.concat([control_dfs[40], treatment_dfs[40]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check: Verify value_counts data for `time_frame = 0` is mirror image of value_counts data for `time_frame = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Combination Counts:\n",
      "    Time_Frame  Treatment  Count\n",
      "0          0.0        0.0    360\n",
      "1          0.0        1.0    190\n",
      "2          0.0        2.0     70\n",
      "3          0.0        3.0     60\n",
      "4          0.0        4.0     10\n",
      "5          0.0        5.0     10\n",
      "6          0.0       10.0     10\n",
      "7          0.0       20.0     10\n",
      "8          1.0        0.0    360\n",
      "9          1.0        1.0    190\n",
      "10         1.0        2.0     70\n",
      "11         1.0        3.0     60\n",
      "12         1.0        4.0     10\n",
      "13         1.0        5.0     10\n",
      "14         1.0       10.0     10\n",
      "15         1.0       20.0     10\n"
     ]
    }
   ],
   "source": [
    "# Verify equal number of time_frame = 0 and time_frame = 1\n",
    "combination_counts = merged10df.groupby(['Time_Frame', 'Treatment']).size().reset_index(name='Count')\n",
    "\n",
    "print(\"Unique Combination Counts:\")\n",
    "print(combination_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Followers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in follower count data and merge with dataframes\n",
    "followers = pd.read_csv(\"Follower-count_Pivot - Sheet1.csv\",)\n",
    "followers['Followers'] = followers['Followers'].astype(int)\n",
    "followers['Profile'] = followers['Profile'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers5df = pd.merge(merged5df, followers, on='Profile', how='left')\n",
    "followers10df = pd.merge(merged10df, followers, on='Profile', how='left')\n",
    "followers20df = pd.merge(merged20df, followers, on='Profile', how='left')\n",
    "followers30df = pd.merge(merged30df, followers, on='Profile', how='left')\n",
    "followers40df = pd.merge(merged40df, followers, on='Profile', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Number of Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data csv\n",
    "file_path = \"data\\SocialInsider_Instagram_posts.csv\"\n",
    "main_df = pd.read_csv(file_path)\n",
    "num_posts = dict(main_df['Profile'].value_counts())\n",
    "posts_df = pd.DataFrame(list(num_posts.items()), columns=['Profile','num_posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final5df = pd.merge(followers5df, posts_df, on='Profile', how='left')\n",
    "final10df = pd.merge(followers10df, posts_df, on='Profile', how='left')\n",
    "final20df = pd.merge(followers20df, posts_df, on='Profile', how='left')\n",
    "final30df = pd.merge(followers30df, posts_df, on='Profile', how='left')\n",
    "final40df = pd.merge(followers40df, posts_df, on='Profile', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Month Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final5df['Month'] = final5df['Date'].dt.month\n",
    "final10df['Month'] = final10df['Date'].dt.month\n",
    "final20df['Month'] = final20df['Date'].dt.month\n",
    "final30df['Month'] = final30df['Date'].dt.month\n",
    "final40df['Month'] = final40df['Date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "#### Save CSVs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dfs = [final5df, final10df, final20df, final30df, final40df]\n",
    "window_sizes = [\"5\",\"10\",\"20\",\"30\",\"40\"]\n",
    "\n",
    "for i in range(len(final_dfs)):\n",
    "    subset_df = final_dfs[i][['Profile','Type', 'engagement_zscore', 'Time_Frame', 'Treatment', 'Followers', 'Month', 'num_posts']]\n",
    "    file_name = \"window\" + window_sizes[i] + \"_regressiondata.csv\"\n",
    "    subset_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check: Read one of the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profile</th>\n",
       "      <th>Type</th>\n",
       "      <th>sqrt_engagement_zscore</th>\n",
       "      <th>Time_Frame</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Month</th>\n",
       "      <th>num_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lenskart</td>\n",
       "      <td>CAROUSEL_ALBUM</td>\n",
       "      <td>0.660412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045041</td>\n",
       "      <td>5</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lenskart</td>\n",
       "      <td>CAROUSEL_ALBUM</td>\n",
       "      <td>0.937294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045041</td>\n",
       "      <td>5</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lenskart</td>\n",
       "      <td>IMAGE</td>\n",
       "      <td>0.287251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045041</td>\n",
       "      <td>5</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lenskart</td>\n",
       "      <td>reel</td>\n",
       "      <td>-0.680130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045041</td>\n",
       "      <td>5</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lenskart</td>\n",
       "      <td>reel</td>\n",
       "      <td>-0.651569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045041</td>\n",
       "      <td>5</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>fashionphile</td>\n",
       "      <td>CAROUSEL_ALBUM</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>517955</td>\n",
       "      <td>4</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>fashionphile</td>\n",
       "      <td>CAROUSEL_ALBUM</td>\n",
       "      <td>-0.262065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>517955</td>\n",
       "      <td>4</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>fashionphile</td>\n",
       "      <td>reel</td>\n",
       "      <td>-0.520655</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>517955</td>\n",
       "      <td>4</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>fashionphile</td>\n",
       "      <td>reel</td>\n",
       "      <td>-0.610770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>517955</td>\n",
       "      <td>4</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>fashionphile</td>\n",
       "      <td>reel</td>\n",
       "      <td>-0.335710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>517955</td>\n",
       "      <td>4</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>960 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Profile            Type  sqrt_engagement_zscore  Time_Frame  \\\n",
       "0        lenskart  CAROUSEL_ALBUM                0.660412         0.0   \n",
       "1        lenskart  CAROUSEL_ALBUM                0.937294         0.0   \n",
       "2        lenskart           IMAGE                0.287251         0.0   \n",
       "3        lenskart            reel               -0.680130         0.0   \n",
       "4        lenskart            reel               -0.651569         0.0   \n",
       "..            ...             ...                     ...         ...   \n",
       "955  fashionphile  CAROUSEL_ALBUM               -0.377025         1.0   \n",
       "956  fashionphile  CAROUSEL_ALBUM               -0.262065         1.0   \n",
       "957  fashionphile            reel               -0.520655         1.0   \n",
       "958  fashionphile            reel               -0.610770         1.0   \n",
       "959  fashionphile            reel               -0.335710         1.0   \n",
       "\n",
       "     Treatment  Followers  Month  num_posts  \n",
       "0          0.0    1045041      5        857  \n",
       "1          0.0    1045041      5        857  \n",
       "2          0.0    1045041      5        857  \n",
       "3          0.0    1045041      5        857  \n",
       "4          0.0    1045041      5        857  \n",
       "..         ...        ...    ...        ...  \n",
       "955        1.0     517955      4        417  \n",
       "956        1.0     517955      4        417  \n",
       "957        1.0     517955      4        417  \n",
       "958        1.0     517955      4        417  \n",
       "959        1.0     517955      4        417  \n",
       "\n",
       "[960 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('window40_regressiondata.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
